# Reddit Comment Extraction (Best-effort, No OAuth)

**Post Title:** How are you automating 1,000+ product showcase videos from photos?
**Post URL:** https://www.reddit.com/r/automation/comments/1q9xvhw/how_are_you_automating_1000_product_showcase/
**JSON URL:** https://www.reddit.com/r/automation/comments/1q9xvhw.json?raw_json=1&sort=top&limit=300&depth=10

## Coverage metrics

- **expected_num_comments:** 9
- **extracted_unique_comments:** 9
- **remaining_pending_child_ids:** 0
- **missing_children_ids:** 0
- **coverage_estimate:** 1.0
- **stop_reason:** completed
- **requests_total:** 1
- **requests_morechildren:** 0

## Analysis instructions

- Filter baseline: `score >= 3` (unless signal is strong)
- Keep parent context when a reply is selected
- Run free solution check on top visible suggestions

## Selected comments (2)

### t1_nz2ub7t (score=1, depth=0)
- **Author:** u/RobbyInEver
- **Parent:** t3_1q9xvhw
- **Link:** https://www.reddit.com/r/automation/comments/1q9xvhw/how_are_you_automating_1000_product_showcase/nz2ub7t/

> Task scheduler running API's to various video generators that have API's (eg. Runway) then stitch sound, text and voice using ffmpeg based automators and finally using another api to upscale to 2k and 4k.
> 
> Titles are created from script using imagickmagick and PHP. Music is AI generated to match either length or mood (if mood then it's just a repeating loop).
> 
> Source videos are cropped or canvas expanded if necessary to fit on portrait (eg. TikTok), square (Instagram) and horizontal (YouTube) formats.
> 
> Ffmpeg also used to modify sounds levels at each point in time (eg. By 30%) whenever the AI voiceover speaks.
> 
> At the end ffmpeg also used to insert both metadata and thumbnails into each video, and at all stages write the status to an online myqsl DB, so that errors and flags can be sent via email if anything jams (mostly due to all backup gen platforms not working when one goes down).
> 
> For more complicated videos, pipeline is paused while an automated prompt (usually email) is sent with link to approve or reject (eg. Video generated before music, titling and voiceover insertion). If rejected the process for that stage is simply repeated.
> 
> Each video we produce in this pipeline takes anywhere from 2 to 6 hours to render (shorter if there's little movement or no need to upscale beyond 2k).

### t1_nyymfqy (score=1, depth=0)
- **Author:** u/stacktrace_wanderer
- **Parent:** t3_1q9xvhw
- **Link:** https://www.reddit.com/r/automation/comments/1q9xvhw/how_are_you_automating_1000_product_showcase/nyymfqy/

> We looked at something similar when marketing wanted volume without turning it into a manual edit factory. What helped was getting ruthless about templates and constraints, same camera moves, same clip order, same text rules, then letting automation fill the slots. The moment we allowed per product creativity, it broke down fast and needed human cleanup. In our case, batching and deterministic inputs mattered more than squeezing quality out of each clip. It did not look cinematic, but it was predictable and scaled without constant babysitting. I would focus less on the video model and more on locking the workflow so it cannot drift.
